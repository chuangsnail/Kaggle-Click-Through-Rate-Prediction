{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65833bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import re\n",
    "import joblib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c7313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def StoreRdmSample():\n",
    "    ## Load part of the training sample because the size is too large\n",
    "    random.seed(4321)\n",
    "    no_lines = 40428967\n",
    "    size = 4 * 10**6\n",
    "    skip = sorted(random.sample(range(1, no_lines), no_lines-size))\n",
    "    train_df = pd.read_csv( \"data/train.csv\", header=0, skiprows=skip )\n",
    "    train_df.to_csv(\"post_data/sampling.csv\", index=False)\n",
    "\n",
    "def AllUnique():\n",
    "    f = open(\"data/train.csv\",'r')\n",
    "    line1 = f.readline()\n",
    "    line1 = line1.strip('\\n')\n",
    "    header_list = re.split(',', line1)\n",
    "    f.close()\n",
    "\n",
    "    fp = open(\"feature_unique.txt\", \"a\")\n",
    "    fp2 = open(\"feature_No.txt\", \"a\")\n",
    "    for c in header_list:\n",
    "        if c == \"id\": continue\n",
    "        df = pd.read_csv(\"data/train.csv\", usecols=[c])\n",
    "        uni_list = df[c].unique()\n",
    "        fp.write( c + \":\" )\n",
    "        fp.write( str(len(uni_list)) + \"\\n\" )\n",
    "        fp.write( str( uni_list ) )\n",
    "        fp.write( \"\\n\\n\" )\n",
    "        fp2.write( c + \":\" )\n",
    "        fp2.write( str(len(uni_list)) + \"\\n\" )\n",
    "    fp.close()\n",
    "    fp2.close()\n",
    "def SeperateTrainTest( df, num_train, _seed=9876 ):\n",
    "    num_train_test = len(df)\n",
    "    random.seed(_seed)\n",
    "    shuffle_no_list = [x for x in range(num_train_test)]\n",
    "    random.shuffle( shuffle_no_list ) # random.shuffle() has no return object\n",
    "    train_no_list = shuffle_no_list[:num_train]\n",
    "    test_no_list = shuffle_no_list[num_train:]\n",
    "    train_df = df.iloc[train_no_list,:]\n",
    "    test_df = df.iloc[test_no_list,:]\n",
    "    train_df.index = range(len(train_df))\n",
    "    test_df.index = range(len(test_df))\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "#StoreRdmSample()\n",
    "\n",
    "### All are categories type, so to see the categories # of all variables\n",
    "#AllUnique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7b98b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_df = pd.read_csv(\"post_data/train_sampling.csv\")\\ntest_df = pd.read_csv(\"post_data/test_sampling.csv\")\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sampling_df = pd.read_csv(\"post_data/sampling.csv\")\n",
    "#train_df, test_df = SeperateTrainTest(sampling_df, 2* 10**6)\n",
    "\n",
    "#train_df.to_csv(\"post_data/train_sampling.csv\", index=False)\n",
    "#test_df.to_csv(\"post_data/test_sampling.csv\", index=False)\n",
    "\n",
    "#del sampling_df\n",
    "\"\"\"\n",
    "train_df = pd.read_csv(\"post_data/train_sampling.csv\")\n",
    "test_df = pd.read_csv(\"post_data/test_sampling.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a45ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransHour( df ):\n",
    "    def EncodingHour( hour ):\n",
    "    ### to 'hour' and 'weekday', 2 features\n",
    "    ### Cut the hour in 8 range [1-4, 4-7, 7-10, 10-13, 13-16, 16-19, 19-22, 22-1]\n",
    "        hour = str(hour)\n",
    "        hour_str = \"20\" + hour[:6]\n",
    "        day = datetime.strptime( hour_str, \"%Y%m%d\").weekday()\n",
    "        day_hour = int(hour[6:])\n",
    "        str_hour = None\n",
    "\n",
    "        if day_hour in [1,2,3]: str_hour = str('1-4')\n",
    "        elif day_hour in [4,5,6]: str_hour = str('4-7')\n",
    "        elif day_hour in [7,8,9]: str_hour = str('7-10') \n",
    "        elif day_hour in [10,11,12]: str_hour = str('10-13') \n",
    "        elif day_hour in [13,14,15]: str_hour = str('13-16') \n",
    "        elif day_hour in [16,17,18]: str_hour = str('16-19') \n",
    "        elif day_hour in [19,20,21]: str_hour = str('19-22') \n",
    "        elif day_hour in [22,23,0]: str_hour = str('22-1')\n",
    "        #print(\"day:\", day)\n",
    "        return day, str_hour\n",
    "    \n",
    "    weekday_list = ['weekday_0.0', 'weekday_1.0', 'weekday_2.0', 'weekday_3.0', 'weekday_4.0', 'weekday_5.0', 'weekday_6.0']\n",
    "    dayhour_list = ['dayhour_1-4', 'dayhour_4-7', 'dayhour_7-10', 'dayhour_10-13', 'dayhour_13-16', 'dayhour_16-19', 'dayhour_19-22', 'dayhour_22-1']\n",
    "    \n",
    "    weekday_ser = pd.Series( np.zeros(shape=(len(df),)) )\n",
    "    dayhour_ser = pd.Series( np.zeros(shape=(len(df),)) )\n",
    "    for i in range(len(df)):\n",
    "        weekday_ser[i], dayhour_ser[i] = EncodingHour(df.loc[i,'hour'])\n",
    "        #print(weekday_ser[i])\n",
    "        #print(dayhour_ser[i])\n",
    "    df.loc[:,'weekday'] = weekday_ser\n",
    "    df.loc[:,'dayhour'] = dayhour_ser   \n",
    "    df['weekday'] = df['weekday'].astype('str')\n",
    "    df['dayhour'] = df['dayhour'].astype('category')\n",
    "    df = df.drop('hour', axis=1)\n",
    "    \n",
    "    df_dayhour_dum = pd.get_dummies( df['dayhour'], prefix=\"dayhour\" )\n",
    "    df_weekday_dum = pd.get_dummies( df['weekday'], prefix=\"weekday\" )\n",
    "    \n",
    "    ## check if there are lack in weekday and dayhour\n",
    "    for x in dayhour_list:\n",
    "        if x not in df_dayhour_dum.columns:\n",
    "            df_dayhour_dum.loc[:,x] = pd.Series( np.zeros(shape=(len(df_dayhour_dum),)) )\n",
    "    for x in weekday_list:\n",
    "        if x not in df_weekday_dum.columns:\n",
    "            df_weekday_dum.loc[:,x] = pd.Series( np.zeros(shape=(len(df_weekday_dum),)) )\n",
    "    ## re-order the dummies                                \n",
    "    df_dayhour_dum = df_dayhour_dum[dayhour_list]\n",
    "    df_weekday_dum = df_weekday_dum[weekday_list]\n",
    "                                                \n",
    "    df = pd.concat( [df, df_dayhour_dum], axis=1 )\n",
    "    df = pd.concat( [df, df_weekday_dum], axis=1 )\n",
    "    \n",
    "    df.drop(['dayhour'], axis=1, inplace=True)\n",
    "    df.drop(['weekday'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0dfe17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Addressing hour first\n",
    "#train_df.info()\n",
    "\n",
    "#testing_train_df = train_df[:1000]\n",
    "#testing_train_df = TransHour(testing_train_df)\n",
    "\n",
    "#train_df = TransHour(train_df)\n",
    "#test_df = TransHour(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072af3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_Cat_df = pd.read_csv(\"post_data/train_Cat_sampling.csv\")\\nself_test_df = pd.read_csv(\"post_data/self_test_sampling.csv\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_Cat_df, self_test_df = SeperateTrainTest(test_df, 10**6, 8765)\n",
    "\n",
    "#train_Cat_df.to_csv(\"post_data/train_Cat_sampling.csv\")\n",
    "#self_test_df.to_csv(\"post_data/self_test_sampling.csv\")\n",
    "#del test_df\n",
    "\"\"\"\n",
    "train_Cat_df = pd.read_csv(\"post_data/train_Cat_sampling.csv\")\n",
    "self_test_df = pd.read_csv(\"post_data/self_test_sampling.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a5cf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_hour_data = train_df[[\\'weekday_0.0\\', \\'weekday_1.0\\', \\'weekday_2.0\\', \\'weekday_3.0\\', \\'weekday_4.0\\', \\'weekday_5.0\\', \\'weekday_6.0\\', \\'dayhour_1-4\\', \\'dayhour_4-7\\', \\'dayhour_7-10\\', \\'dayhour_10-13\\', \\'dayhour_13-16\\', \\'dayhour_16-19\\', \\'dayhour_19-22\\', \\'dayhour_22-1\\']]\\ntest_hour_data = self_test_df[[\\'weekday_0.0\\', \\'weekday_1.0\\', \\'weekday_2.0\\', \\'weekday_3.0\\', \\'weekday_4.0\\', \\'weekday_5.0\\', \\'weekday_6.0\\', \\'dayhour_1-4\\', \\'dayhour_4-7\\', \\'dayhour_7-10\\', \\'dayhour_10-13\\', \\'dayhour_13-16\\', \\'dayhour_16-19\\', \\'dayhour_19-22\\', \\'dayhour_22-1\\']]\\ntrain_hour_data.to_csv(\"post_data/train_hour_data.csv\", index=False)\\ntest_hour_data.to_csv(\"post_data/test_hour_data.csv\", index=False)\\ndel train_hour_data\\ndel test_hour_data\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store time data respectively\n",
    "\"\"\"\n",
    "train_hour_data = train_df[['weekday_0.0', 'weekday_1.0', 'weekday_2.0', 'weekday_3.0', 'weekday_4.0', 'weekday_5.0', 'weekday_6.0', 'dayhour_1-4', 'dayhour_4-7', 'dayhour_7-10', 'dayhour_10-13', 'dayhour_13-16', 'dayhour_16-19', 'dayhour_19-22', 'dayhour_22-1']]\n",
    "test_hour_data = self_test_df[['weekday_0.0', 'weekday_1.0', 'weekday_2.0', 'weekday_3.0', 'weekday_4.0', 'weekday_5.0', 'weekday_6.0', 'dayhour_1-4', 'dayhour_4-7', 'dayhour_7-10', 'dayhour_10-13', 'dayhour_13-16', 'dayhour_16-19', 'dayhour_19-22', 'dayhour_22-1']]\n",
    "train_hour_data.to_csv(\"post_data/train_hour_data.csv\", index=False)\n",
    "test_hour_data.to_csv(\"post_data/test_hour_data.csv\", index=False)\n",
    "del train_hour_data\n",
    "del test_hour_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0c6ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### classify vars to be lists\n",
    "## A.  categories<10\n",
    "## B.  100>categories>10 and with importance through human knowledge\n",
    "## C.  10000>categories>100\n",
    "## D.  categories>100000\n",
    "A_list = ['C1', 'banner_pos', 'device_type', 'device_conn_type', 'C15', 'C16', 'C18']\n",
    "B_list = ['site_category', 'app_category']\n",
    "C_list = ['site_id', 'site_domain', 'app_id', 'app_domain', 'device_model', 'C14', 'C17', 'C19', 'C20', 'C21']\n",
    "D_list = ['device_id', 'device_ip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f23512c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeFtCatsDF( df, ft ):\n",
    "\n",
    "    df_ct = pd.DataFrame(df[ft].value_counts())\n",
    "    df_ct.columns = [\"counts\"]  # just 1 columns so far\n",
    "    df_ct[\"id\"] = df_ct.index\t# col == 1\n",
    "    df_ct[\"ratio\"] = pd.Series( np.zeros(shape=(len(df_ct),)) ) # col == 2\n",
    "    df_ct.index = range(len(df_ct))\n",
    "\n",
    "    for x, i in zip(df_ct[\"id\"], df_ct.index):\n",
    "        tmp_df = df[df[ft]==x]\n",
    "        No_nonclick = len(tmp_df[tmp_df[\"click\"]==0])\n",
    "        if No_nonclick == 0:\n",
    "            ratio = 0.\n",
    "        else:\n",
    "            ratio = len(tmp_df[tmp_df[\"click\"]==1])/No_nonclick\n",
    "        df_ct.loc[i,\"ratio\"] = ratio\n",
    "    return df_ct\n",
    "\n",
    "def PlotFtCats( df, ft_list ):\n",
    "\n",
    "    def PlotOneFt( df, ft ):\n",
    "        df_ct = MakeFtCatsDF( df, ft )\n",
    "        total_click = len(df[df[\"click\"]==1])\n",
    "        average_ratio = total_click/(len(df)-total_click)\n",
    "        \n",
    "        len_df_ct = len(df_ct)\n",
    "        bin_list = range(0,len_df_ct,1)\n",
    "        \n",
    "        fig = plt.figure(figsize=(12,32), dpi=120)\n",
    "        sb1 = fig.add_subplot(311)\n",
    "        plt.hist( df_ct.index, weights=df_ct[\"counts\"], color='grey', cumulative=False, alpha=0.5, bins=bin_list)\n",
    "        plt.title( \"Counts of category [\" + ft + \"]\" )\n",
    "        sb1.set_yscale('log')\n",
    "        \n",
    "        sb2 = fig.add_subplot(312)\n",
    "        plt.hist( df_ct.index, weights=df_ct[\"ratio\"], color='blue', cumulative=False, alpha=0.6, histtype='step', bins=bin_list)\n",
    "        plt.axhline( y=average_ratio, color='r', linestyle='-')\n",
    "        plt.title( \"Click/Non-click ratio [\" + ft + \"]\" )\n",
    "        \n",
    "        thirty_percent_cat = int(len(df_ct)/3)\n",
    "        sb3 = fig.add_subplot(313)\n",
    "        plt.hist( df_ct.index[:thirty_percent_cat], weights=df_ct[\"ratio\"][:thirty_percent_cat], color='blue', cumulative=False, alpha=0.6, histtype='step', bins=bin_list[:thirty_percent_cat])\n",
    "        plt.axhline( y=average_ratio, color='r', linestyle='-')\n",
    "        plt.title( \"Click/Non-click ratio (head-30% categories) [\" + ft + \"]\" )\n",
    "        sb3.set_yscale('log')\n",
    "\n",
    "        plt.show()\n",
    "        figname = \"Fig/\" + ft + \"_Catscounts_CNratio.png\"\n",
    "        fig.savefig( figname )\n",
    "\n",
    "    for ft in ft_list:\n",
    "        PlotOneFt(df, ft)\n",
    "\n",
    "## Deal with C type\n",
    "#PlotFtCats( train_df, C_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e08179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "def TransToOneFourth( ft, df ):\n",
    "# [ \"HIGH\", \"MIDHIGH\", \"MIDLOW\", \"LOW\", \"OTHER\" ]\n",
    "    \n",
    "    df_ct = MakeFtCatsDF( df, ft )\n",
    "    total_click = len(df[df[\"click\"]==1])\n",
    "    mean_ratio = total_click/(len(df)-total_click)\n",
    "    sm_ratio = 0.\n",
    "    for i in df_ct.index:\n",
    "        sm_ratio += (df_ct.loc[i][\"ratio\"])**2 * df_ct.loc[i][\"counts\"]\n",
    "    sm_ratio /= len(df)\n",
    "    sigma_ratio = math.sqrt( sm_ratio - mean_ratio**2 )/math.sqrt(2.)\n",
    "\n",
    "    print(\"Ft-\" + ft +\", mean_ratio: \", mean_ratio)\n",
    "    print(\"Ft-\" + ft +\", sigma_ratio: \", sigma_ratio)\n",
    "\n",
    "    df_high = df_ct[ df_ct[\"ratio\"] >= (mean_ratio+sigma_ratio) ]\n",
    "    df_midh = df_ct[ (df_ct[\"ratio\"]<(mean_ratio+sigma_ratio)) & (df_ct[\"ratio\"]>=mean_ratio) ]\n",
    "    df_midl = df_ct[ (df_ct[\"ratio\"]>=(mean_ratio-sigma_ratio)) & (df_ct[\"ratio\"]<mean_ratio) ]\n",
    "    df_low = df_ct[ df_ct[\"ratio\"] < (mean_ratio-sigma_ratio) ]\n",
    "\n",
    "    high_list = df_high[\"id\"].to_list()\n",
    "    midh_list = df_midh[\"id\"].to_list()\n",
    "    midl_list = df_midl[\"id\"].to_list()\n",
    "    low_list = df_low[\"id\"].to_list()\n",
    "\n",
    "    # Store for transforming the click-unknown testing sample \n",
    "    with open(\"Lists/HIGHlist_\" + ft, \"w\") as f1:\n",
    "        json.dump( high_list ,f1 )\n",
    "    with open(\"Lists/MIDHIGHlist_\" + ft, \"w\") as f2:\n",
    "        json.dump( midh_list ,f2 )\n",
    "    with open(\"Lists/MIDLOWlist_\" + ft, \"w\") as f3:\n",
    "        json.dump( midl_list ,f3 )\n",
    "    with open(\"Lists/LOWlist_\" + ft, \"w\") as f4:\n",
    "        json.dump( low_list ,f4 )\n",
    "    \n",
    "    print(\"Ft-\" + ft +\", Finish storing 4 category lists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "245416b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropNoUse(df):\n",
    "    df.drop([\"id\"], axis=1, inplace=True)\n",
    "    df.drop(drop_C_list, axis=1, inplace=True)\n",
    "    df.drop(D_list, axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18bd7f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New C_list: ['site_id', 'site_domain', 'app_id', 'app_domain', 'device_model', 'C14']\n"
     ]
    }
   ],
   "source": [
    "## By observation, I decide to drop the ['C17', 'C19', 'C20', 'C21']\n",
    "drop_C_list = ['C17', 'C19', 'C20', 'C21']\n",
    "\n",
    "C_list = [x for x in C_list if x not in drop_C_list]\n",
    "print(\"New C_list:\", C_list)\n",
    "\n",
    "## for training\n",
    "#for ft in C_list:\n",
    "#    TransToOneFourth(ft, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31bacb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_df.info()\\ntrain_df = DropNoUse(train_df)\\nself_test_df = DropNoUse(self_test_df)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_df.info()\n",
    "train_df = DropNoUse(train_df)\n",
    "self_test_df = DropNoUse(self_test_df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b67694f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deal with B_list\n",
    "## Do RF on the addition of the 2 'category' variables' one-hot encoding\n",
    "## output should be the probability\n",
    "\n",
    "def StoreRFTrain( df ):\n",
    "    df = df[[\"site_category\",\"app_category\",\"click\"]]\n",
    "    df.info()\n",
    "    RF_ft_train = pd.DataFrame(df['click'])\n",
    "    for ft in B_list:\n",
    "        df[ft] = df[ft].astype(\"category\")\n",
    "        dum_ft_train = pd.get_dummies( df[ft], prefix=ft )\n",
    "        RF_ft_train = pd.concat([RF_ft_train, dum_ft_train], axis=1)\n",
    "    RF_ft_train.info()\n",
    "    ## store the training feature for later applying\n",
    "    print(\"RF_ft_train.columns\", RF_ft_train.columns)\n",
    "    with open(\"Lists/RF_fts.txt\", \"w\") as f:\n",
    "        json.dump( RF_ft_train.columns.to_list() ,f ) # the 1st column is 'click'\n",
    "    \n",
    "    RF_ft_train.to_csv(\"post_data/dum_RF_train.csv\", index=False)\n",
    "    print( \"Finish Storing dum training info for RF!\")\n",
    "\n",
    "def RFCatTrain( N_est=200, crit='gini', rdmstate=1, nj=-1 ):\n",
    "    dum_feature = pd.read_csv(\"post_data/dum_RF_train.csv\")\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    forest = RandomForestClassifier(n_estimators=N_est, criterion=crit, random_state=rdmstate, n_jobs=nj)\n",
    "    forest.fit( dum_feature.iloc[:,1:], dum_feature['click'] ) # columns index=0 is 'click'\n",
    "    print( \"Finish RF fit!\")\n",
    "    \n",
    "    model_name = \"Model/RFCat_\" + str(N_est) + \"_\" + crit + \"_\" + str(rdmstate) + \"_\" + str(nj) \n",
    "    joblib.dump( forest, model_name )\n",
    "    print(\"Finish training RF Cat!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebd119f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## seperate half test_df to be the training sample for RF\n",
    "## for training!\n",
    "#train_Cat_df.info()\n",
    "#StoreRFTrain(train_Cat_df)\n",
    "#RFCatTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d96b4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to apply OneFourth on other df\n",
    "def OneFourth_Ft( ft, df ):\n",
    "\n",
    "    high_list = []\n",
    "    midh_list = []\n",
    "    midl_list = []\n",
    "    low_list = [] \n",
    "\n",
    "    with open(\"Lists/HIGHlist_\" + ft, \"r\") as f1:\n",
    "        high_list = json.load( f1 )\n",
    "    with open(\"Lists/MIDHIGHlist_\" + ft, \"r\") as f2:\n",
    "        midh_list = json.load( f2 )\n",
    "    with open(\"Lists/MIDLOWlist_\" + ft, \"r\") as f3:\n",
    "        midl_list = json.load( f3 )\n",
    "    with open(\"Lists/LOWlist_\" + ft, \"r\") as f4:\n",
    "        low_list = json.load( f4 )\n",
    "\n",
    "    for x in high_list:\n",
    "        df.loc[ df[ft]==x, ft ] = \"HIGH\"\n",
    "    for x in midh_list:\n",
    "        df.loc[ df[ft]==x, ft ] = \"MIDHIGH\"\n",
    "    for x in midl_list:\n",
    "        df.loc[ df[ft]==x, ft ] = \"MIDLOW\"\n",
    "    for x in low_list:\n",
    "        df.loc[ df[ft]==x, ft ] = \"LOW\"\n",
    "    df.loc[ (df[ft]!=\"HIGH\")&(df[ft]!=\"MIDHIGH\")&(df[ft]!=\"MIDLOW\")&(df[ft]!=\"LOW\"), ft ] = \"OTHER\"\n",
    "\n",
    "    dum_ft_df = pd.get_dummies(df[ft], prefix=ft)\n",
    "    if ft+\"_OTHER\" in dum_ft_df.columns:\n",
    "        dum_ft_df.drop([ft+\"_OTHER\"], axis=1, inplace=True) # \"OTHER\" would appear only in testing sample, not in training sample\n",
    "    df.drop([ft], axis=1, inplace=True)\n",
    "    df = pd.concat([df, dum_ft_df], axis=1)\n",
    "    \n",
    "    print(\"Finish dealing with feature > \" + ft + \"!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# to apply the trained RF on other df\n",
    "def RF_Ft(df):\n",
    "    df_dum_site = pd.get_dummies( df['site_category'], prefix='site_category' )\n",
    "    df_dum_app = pd.get_dummies( df['app_category'], prefix='app_category' )\n",
    "    df_dum = pd.concat([df_dum_site, df_dum_app], axis=1)\n",
    "    \n",
    "    main_col_list = []\n",
    "    with open(\"Lists/RF_fts.txt\", \"r\") as f:\n",
    "        main_col_list = json.load( f )\n",
    "    ## main_col_list[0] == \"click\"\n",
    "    main_col_list = main_col_list[1:]\n",
    "\n",
    "    input_list = df_dum.columns.to_list()\n",
    "    for x in main_col_list:\n",
    "        if x not in input_list:\n",
    "            df_dum[x] = pd.Series( np.zeros(shape=(len(df_dum),)) )\n",
    "\n",
    "    df_dum = df_dum[main_col_list]\n",
    "    print(main_col_list)\n",
    "    print(df_dum.columns.to_list())\n",
    "    \n",
    "    forest = joblib.load( \"Model/RFCat_200_gini_1_-1\" )\n",
    "    rf_ser = forest.predict( df_dum )\n",
    "    df[\"RFCat\"] = rf_ser\n",
    "    df.drop(B_list, axis=1, inplace=True)\n",
    "    \n",
    "    print(\"Finish transforming RF!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def TransOneHotA_train(df): # for training\n",
    "    A_fts_list = []\n",
    "    dum_df = pd.DataFrame()\n",
    "    for ft in A_list:\n",
    "        dum_ft = pd.get_dummies( df[ft], prefix=ft )\n",
    "        #dum_ele = dum_ft.columns.to_list()\n",
    "        #A_fts_list = [*A_fts_list, *dum_ele]\n",
    "        df.drop([ft], axis=1, inplace=True)\n",
    "        dum_df = pd.concat([dum_df, dum_ft], axis=1)\n",
    "    df = pd.concat([df, dum_df], axis=1)\n",
    "    with open(\"Lists/A_fts.txt\", \"w\") as f:\n",
    "        json.dump( dum_df.columns.to_list() ,f )\n",
    "    print(\"Finish One-Hot A!\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def TransOneHotA(df): # for testing \n",
    "    with open(\"Lists/A_fts.txt\", \"r\") as f:\n",
    "        A_fts_list = json.dump( f )\n",
    "\n",
    "    dum_df = pd.DataFrame()\n",
    "    for ft in A_list:    \n",
    "        dum_ft = pd.get_dummies( df[ft], prefix=ft )\n",
    "        df.drop([ft], axis=1, inplace=True)\n",
    "        dum_df = pd.concat([dum_df, dum_ft], axis=1)\n",
    "        \n",
    "    for x in A_fts_list:\n",
    "        if x not in df:\n",
    "            dum_df[x] = pd.Series( np.zeros(shape=(len(df_dum),)) )\n",
    "    \n",
    "    dum_df = dum_df[A_fts_list]\n",
    "    df = pd.concat([df, dum_df], axis=1)\n",
    "    \n",
    "    print(\"Finish One-Hot A!\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f43937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_train_df = train_df[:1000]\n",
    "#testing_train_df = TransOneHotA_train(testing_train_df)\n",
    "#for ft in C_list:\n",
    "#    testing_train_df = OneFourth_Ft(ft, testing_train_df)\n",
    "#testing_train_df = RF_Ft(testing_train_df)\n",
    "#testing_train_df.info()\n",
    "#print(testing_train_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "922d94f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_df = TransOneHotA_train(train_df)\\nfor ft in C_list:\\n    train_df = OneFourth_Ft(ft, train_df)\\ntrain_df = RF_Ft(train_df)\\ntrain_df.info()\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_df = TransOneHotA_train(train_df)\n",
    "for ft in C_list:\n",
    "    train_df = OneFourth_Ft(ft, train_df)\n",
    "train_df = RF_Ft(train_df)\n",
    "train_df.info()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f658b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(\"post_data/train_Ft.csv\")\n",
    "#train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d0f79b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nself_test_df = TransOneHotA(self_test_df)\\nfor ft in C_list:\\n    self_test_df = OneFourth_Ft(ft, self_test_df)\\nself_test_df = RF_Ft(self_test_df)\\nself_test_df.info()\\nself_test_df.to_csv(\"post_data/self_test_Ft.csv\")\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "self_test_df = TransOneHotA(self_test_df)\n",
    "for ft in C_list:\n",
    "    self_test_df = OneFourth_Ft(ft, self_test_df)\n",
    "self_test_df = RF_Ft(self_test_df)\n",
    "self_test_df.info()\n",
    "self_test_df.to_csv(\"post_data/self_test_Ft.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee29f147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> step 0\n",
      ">>>>> step 1\n",
      ">>>>> step 2\n",
      ">>>>> step 3\n"
     ]
    }
   ],
   "source": [
    "# deal with the final dealing df\n",
    "print(\">>>>> step\", 0 )\n",
    "value_df = pd.read_csv(\"data/test.csv\")\n",
    "print(\">>>>> step\", 1 )\n",
    "value_df = TransHour(value_df)\n",
    "print(\">>>>> step\", 2 )\n",
    "time_list = ['weekday_0.0', 'weekday_1.0', 'weekday_2.0', 'weekday_3.0', 'weekday_4.0', 'weekday_5.0', 'weekday_6.0', 'dayhour_1-4', 'dayhour_4-7', 'dayhour_7-10', 'dayhour_10-13', 'dayhour_13-16', 'dayhour_16-19', 'dayhour_19-22', 'dayhour_22-1']\n",
    "value_hour_data = value_df[time_list]\n",
    "print(\">>>>> step\", 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c8998cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> step 4\n",
      ">>>>> step 5\n",
      "Finish One-Hot A!\n",
      ">>>>> step 6\n",
      "Finish dealing with feature > site_id!\n",
      "Finish dealing with feature > site_domain!\n",
      "Finish dealing with feature > app_id!\n",
      "Finish dealing with feature > app_domain!\n",
      "Finish dealing with feature > device_model!\n",
      "Finish dealing with feature > C14!\n",
      ">>>>> step 7\n",
      "['site_category_0569f928', 'site_category_28905ebd', 'site_category_335d28a8', 'site_category_3e814130', 'site_category_42a36e14', 'site_category_50e219e0', 'site_category_5378d028', 'site_category_70fb0e29', 'site_category_72722551', 'site_category_75fa27f6', 'site_category_76b2941d', 'site_category_8fd0aea4', 'site_category_9ccfa2ea', 'site_category_a818d37a', 'site_category_bcf865d9', 'site_category_c0dd3be3', 'site_category_dedf689d', 'site_category_e787de0e', 'site_category_f028772b', 'site_category_f66779e6', 'app_category_07d7df22', 'app_category_09481d60', 'app_category_0bfbc358', 'app_category_0d82db25', 'app_category_0f2161f8', 'app_category_0f9a328c', 'app_category_18b1e0be', 'app_category_2281a340', 'app_category_2fc4f2aa', 'app_category_4681bb9d', 'app_category_4ce2e9fc', 'app_category_5326cf99', 'app_category_6fea3693', 'app_category_7113d72a', 'app_category_75d80bbe', 'app_category_79f0b860', 'app_category_879c24eb', 'app_category_8ded1f7a', 'app_category_8df2e842', 'app_category_a3c42688', 'app_category_a7fd01ec', 'app_category_a86a3e89', 'app_category_cef3e649', 'app_category_d1327cf5', 'app_category_dc97ec06', 'app_category_f95efa07', 'app_category_fc6fa53d']\n",
      "['site_category_0569f928', 'site_category_28905ebd', 'site_category_335d28a8', 'site_category_3e814130', 'site_category_42a36e14', 'site_category_50e219e0', 'site_category_5378d028', 'site_category_70fb0e29', 'site_category_72722551', 'site_category_75fa27f6', 'site_category_76b2941d', 'site_category_8fd0aea4', 'site_category_9ccfa2ea', 'site_category_a818d37a', 'site_category_bcf865d9', 'site_category_c0dd3be3', 'site_category_dedf689d', 'site_category_e787de0e', 'site_category_f028772b', 'site_category_f66779e6', 'app_category_07d7df22', 'app_category_09481d60', 'app_category_0bfbc358', 'app_category_0d82db25', 'app_category_0f2161f8', 'app_category_0f9a328c', 'app_category_18b1e0be', 'app_category_2281a340', 'app_category_2fc4f2aa', 'app_category_4681bb9d', 'app_category_4ce2e9fc', 'app_category_5326cf99', 'app_category_6fea3693', 'app_category_7113d72a', 'app_category_75d80bbe', 'app_category_79f0b860', 'app_category_879c24eb', 'app_category_8ded1f7a', 'app_category_8df2e842', 'app_category_a3c42688', 'app_category_a7fd01ec', 'app_category_a86a3e89', 'app_category_cef3e649', 'app_category_d1327cf5', 'app_category_dc97ec06', 'app_category_f95efa07', 'app_category_fc6fa53d']\n",
      "Finish transforming RF!\n",
      ">>>>> step 8\n",
      ">>>>> step 9\n",
      ">>>>> step 10\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4577464 entries, 0 to 4577463\n",
      "Data columns (total 81 columns):\n",
      " #   Column                Dtype  \n",
      "---  ------                -----  \n",
      " 0   C1_1001               uint8  \n",
      " 1   C1_1002               uint8  \n",
      " 2   C1_1005               uint8  \n",
      " 3   C1_1007               uint8  \n",
      " 4   C1_1008               uint8  \n",
      " 5   C1_1010               uint8  \n",
      " 6   C1_1012               uint8  \n",
      " 7   banner_pos_0          uint8  \n",
      " 8   banner_pos_1          uint8  \n",
      " 9   banner_pos_2          uint8  \n",
      " 10  banner_pos_3          uint8  \n",
      " 11  banner_pos_4          uint8  \n",
      " 12  banner_pos_7          uint8  \n",
      " 13  device_type_0         uint8  \n",
      " 14  device_type_1         uint8  \n",
      " 15  device_type_4         uint8  \n",
      " 16  device_type_5         uint8  \n",
      " 17  device_conn_type_0    uint8  \n",
      " 18  device_conn_type_2    uint8  \n",
      " 19  device_conn_type_3    uint8  \n",
      " 20  device_conn_type_5    uint8  \n",
      " 21  C15_120               uint8  \n",
      " 22  C15_216               uint8  \n",
      " 23  C15_300               uint8  \n",
      " 24  C15_320               uint8  \n",
      " 25  C15_480               uint8  \n",
      " 26  C15_728               uint8  \n",
      " 27  C15_768               uint8  \n",
      " 28  C15_1024              uint8  \n",
      " 29  C16_20                uint8  \n",
      " 30  C16_36                uint8  \n",
      " 31  C16_50                uint8  \n",
      " 32  C16_90                uint8  \n",
      " 33  C16_250               uint8  \n",
      " 34  C16_320               uint8  \n",
      " 35  C16_480               uint8  \n",
      " 36  C16_768               uint8  \n",
      " 37  C16_1024              uint8  \n",
      " 38  C18_0                 uint8  \n",
      " 39  C18_1                 uint8  \n",
      " 40  C18_2                 uint8  \n",
      " 41  C18_3                 uint8  \n",
      " 42  site_id_HIGH          uint8  \n",
      " 43  site_id_LOW           uint8  \n",
      " 44  site_id_MIDHIGH       uint8  \n",
      " 45  site_id_MIDLOW        uint8  \n",
      " 46  site_domain_HIGH      uint8  \n",
      " 47  site_domain_LOW       uint8  \n",
      " 48  site_domain_MIDHIGH   uint8  \n",
      " 49  site_domain_MIDLOW    uint8  \n",
      " 50  app_id_HIGH           uint8  \n",
      " 51  app_id_MIDHIGH        uint8  \n",
      " 52  app_id_MIDLOW         uint8  \n",
      " 53  app_domain_HIGH       uint8  \n",
      " 54  app_domain_LOW        uint8  \n",
      " 55  app_domain_MIDHIGH    uint8  \n",
      " 56  app_domain_MIDLOW     uint8  \n",
      " 57  device_model_HIGH     uint8  \n",
      " 58  device_model_LOW      uint8  \n",
      " 59  device_model_MIDHIGH  uint8  \n",
      " 60  device_model_MIDLOW   uint8  \n",
      " 61  C14_HIGH              uint8  \n",
      " 62  C14_LOW               uint8  \n",
      " 63  C14_MIDHIGH           uint8  \n",
      " 64  C14_MIDLOW            uint8  \n",
      " 65  RFCat                 int64  \n",
      " 66  weekday_0.0           float64\n",
      " 67  weekday_1.0           float64\n",
      " 68  weekday_2.0           float64\n",
      " 69  weekday_3.0           float64\n",
      " 70  weekday_4.0           uint8  \n",
      " 71  weekday_5.0           float64\n",
      " 72  weekday_6.0           float64\n",
      " 73  dayhour_1-4           uint8  \n",
      " 74  dayhour_4-7           uint8  \n",
      " 75  dayhour_7-10          uint8  \n",
      " 76  dayhour_10-13         uint8  \n",
      " 77  dayhour_13-16         uint8  \n",
      " 78  dayhour_16-19         uint8  \n",
      " 79  dayhour_19-22         uint8  \n",
      " 80  dayhour_22-1          uint8  \n",
      "dtypes: float64(6), int64(1), uint8(74)\n",
      "memory usage: 567.5 MB\n"
     ]
    }
   ],
   "source": [
    "value_df.drop(time_list, axis=1, inplace=True)\n",
    "print(\">>>>> step\", 4 )\n",
    "value_df = DropNoUse(value_df)\n",
    "print(\">>>>> step\", 5 )\n",
    "value_df = TransOneHotA(value_df)\n",
    "print(\">>>>> step\", 6 )\n",
    "for ft in C_list:\n",
    "    value_df = OneFourth_Ft(ft, value_df)\n",
    "print(\">>>>> step\", 7 )\n",
    "value_df = RF_Ft(value_df)\n",
    "print(\">>>>> step\", 8 )\n",
    "value_df = pd.concat([value_df, value_hour_data], axis=1)\n",
    "print(\">>>>> step\", 9 )\n",
    "value_df.to_csv(\"post_data/value_Ft.csv\")\n",
    "print(\">>>>> step\", 10 )\n",
    "value_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
